It seems like you're discussing the evaluation of a method against another method or baseline, specifically using the Hi4D dataset and focusing on 3DPCK (3D Point Cloud Keypoint Correspondence) metrics with tight thresholds ranging from 0 to 100mm. Here's a breakdown of your statement:

- **Hi4D Dataset**: This is likely a dataset used for evaluating 3D point cloud correspondence tasks, which is common in computer vision and robotics.
- **Tight Thresholds**: These refer to a specific range of distances (from 0 to 100mm) within which keypoint correspondences must be considered correct. A "tight" threshold means that the correspondences need to be very close, making the task more challenging.
- **3DPCK (3D Point Cloud Keypoint Correspondence)**: This metric measures how well keypoints in one point cloud correspond to keypoints in another point cloud. It's a way to evaluate the accuracy of 3D point cloud registration or matching algorithms.

The statement indicates that your method performs significantly better than other methods across all thresholds from 0 to 100mm. This suggests that your method is highly accurate, especially under stringent conditions where correspondences need to be very close together.

The notation of methods follows the conventions established in Table \ref{tab:hi4d_eval}, which likely contains the performance metrics for different methods evaluated on the Hi4D dataset. This table would provide a basis for comparing the performance of various methods, including yours, against each other.

If you have any specific questions about this evaluation or need further clarification on any part of the statement, feel free to ask!