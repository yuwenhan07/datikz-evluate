{
  "prompt": "\\footnotesize\\textbf{Tabular MDP.} The environment starts in state $s_0$ and has horizon $H=1$. Transition probabilities from state $s_0$ are given for the two binary actions $a_0, a_1$ (which send the agent to the other state with complementary probability).",
  "response": "To generate LaTeX code for the image you provided, you can use the `tikz` package along with the `matrix` library to create the diagram. Below is the LaTeX code that produces the image:\n\n```latex\n\\documentclass{article}\n\\usepackage{amsmath}\n\\usepackage{tikz}\n\\usetikzlibrary{matrix}\n\n\\begin{document}\n\n\\begin{figure}[h]\n    \\centering\n    \\begin{tikzpicture}[node distance=2cm]\n        % Define states\n        \\node[state] (s0) {$s_0$};\n        \\node[state, right of=s0] (s1) {$s_1$};\n        \\node[state, below of=s0] (s2) {$s_2$};\n\n        % Draw arrows and labels\n        \\draw[->] (s0) -- node[above left] {$(a_0, 0.8)$} (s1);\n        \\draw[->] (s0) -- node[below left] {$(a_1, 0.6)$} (s2);\n\n        % Add rewards\n        \\node[right of=s1, xshift=1cm] {$R = 1$};\n        \\node[right of=s2, xshift=1cm] {$R = 0$};\n    \\end{tikzpicture}\n    \\caption{Tabular MDP. The environment starts in state $s_0$ and has horizon $H=1$. Transition probabilities from state $s_0$ are given for the two binary actions $a_0, a_1$ (which send the agent to the other state with complementary probability).}\n    \\label{fig:tabular_mdp}\n\\end{figure}\n\n\\end{document}\n```\n\nThis code defines the nodes for the states (`s0`, `s1`, `s2`) and uses the `matrix` library to place them appropriately. Arrows are drawn between the states with their respective transition probabilities. Rewards are added to the right of each terminal state.\n\nMake sure to compile the document using a LaTeX editor that supports TikZ, such as Overleaf or a local LaTeX distribution.",
  "latex_code": "\\documentclass{article}\n\\usepackage{amsmath}\n\\usepackage{tikz}\n\\usetikzlibrary{matrix}\n\n\\begin{document}\n\n\\begin{figure}[h]\n    \\centering\n    \\begin{tikzpicture}[node distance=2cm]\n        % Define states\n        \\node[state] (s0) {$s_0$};\n        \\node[state, right of=s0] (s1) {$s_1$};\n        \\node[state, below of=s0] (s2) {$s_2$};\n\n        % Draw arrows and labels\n        \\draw[->] (s0) -- node[above left] {$(a_0, 0.8)$} (s1);\n        \\draw[->] (s0) -- node[below left] {$(a_1, 0.6)$} (s2);\n\n        % Add rewards\n        \\node[right of=s1, xshift=1cm] {$R = 1$};\n        \\node[right of=s2, xshift=1cm] {$R = 0$};\n    \\end{tikzpicture}\n    \\caption{Tabular MDP. The environment starts in state $s_0$ and has horizon $H=1$. Transition probabilities from state $s_0$ are given for the two binary actions $a_0, a_1$ (which send the agent to the other state with complementary probability).}\n    \\label{fig:tabular_mdp}\n\\end{figure}\n\n\\end{document}",
  "ground_truth": "\\documentclass{article}\n\\usepackage[T1]{fontenc}\n\\usepackage[dvipsnames]{xcolor}\n\\usepackage{tikz}\n\\usetikzlibrary{positioning,arrows.meta}\n\\usepackage{amsmath}\n\\usepackage{amssymb}\n\\usepackage{xcolor}\n\n\\begin{document}\n\n\\begin{tikzpicture}[auto,\nvertex/.style={minimum size=0.8cm,draw,circle,text width=0.8mm},\n]%\n    \\node[vertex,label=center:$s_{0}$] (s0) {};\n    \\node[vertex,above right=1cm and 1cm of s0,label=center:$s_1$] (s1) {}; %\n    \\node[right=0.25cm of s1] (r1) {\\footnotesize $R=1$};\n    \\node[vertex, below right= 1cm and 1cm of s0, label=center:$s_2$] (s2) {};\n    \\node[right=0.25cm of s2] (r2) {\\footnotesize $R=0$};\n\n     \\path[-{Stealth[]}] %\n      (s0) edge node[sloped, anchor=center,align=center,above] {$(a_0, 0.8)$} (s1)\n      (s0) edge node[sloped, anchor=center,align=center,below] {$(a_1, 0.6)$} (s2)\n    ;\n\n\\end{tikzpicture}\n\n\\end{document}"
}