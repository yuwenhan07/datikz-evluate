{
  "prompt": "We show an overview of our method, Forecasting Model Search (FMS), which builds on DyHPO's multifidelity method from Algorithm~\\ref{alg:dyhpo}. Novel components of FMS are highlighted in {\\color{blue}blue} and further detailed in Algorithm~\\ref{alg:fms}. We include DyHPO's features from the hyperparameter configuration, budget, and learning curve \\cite{wistuba2023supervising}. Notably, we also featurize the model's checkpointed weights $\\mathbf{W}$ with a permutation-invariant graph metanetwork (PIGMN) as in Section~\\ref{pigms} for input to a deep kernel GP (see Equation~\\ref{deepkernelgp}/\\ref{eq:weight_kernel}). This provides the HPO with an -- often pre-existing -- rich source of information, which implicitly includes the architecture, dataset, loss, and optimization process. FMS shows improved predictions about hyperparameter performance across compute budgets (see Table~\\ref{ktau-values}), improved quality of the final selected configuration across compute budgets (see Figure~\\ref{fig:regret_over_time}), and a potential to generalize beyond what was seen in training (see Figure~\\ref{fig:fms-transfer}). Specific design choices for this surrogate model are detailed in Appendix Section~\\ref{meta-hyperparams}.",
  "response": "To generate LaTeX code for the provided image and caption, you can use the following structure. Note that the image is a complex diagram, so it's best to use a package like `tikz` or `pgfplots` for drawing the diagram. Below is the LaTeX code for the diagram and the caption:\n\n```latex\n\\documentclass{article}\n\\usepackage{amsmath}\n\\usepackage{graphicx}\n\\usepackage{hyperref}\n\\usepackage{tikz}\n\\usetikzlibrary{arrows.meta, positioning, shapes.geometric}\n\n\\begin{document}\n\n\\begin{figure}[h]\n    \\centering\n    \\begin{tikzpicture}[node distance=1cm, auto]\n        % Nodes\n        \\node[draw, rectangle] (config) {Hyperparameter Config $\\mathbf{x}$};\n        \\node[draw, rectangle, below=of config] (budget) {Budget $r$};\n        \\node[draw, rectangle, below=of budget] (learning_curve) {Learning Curve $\\mathbf{Y}_{1:T}$};\n        \\node[draw, rectangle, below=of learning_curve, fill=blue!20] (checkpoint) {Model Checkpoint $\\mathbf{W}$};\n        \\node[draw, circle, right=of config] (hidden_layer_1) {Hidden Layer 1};\n        \\node[draw, circle, right=of hidden_layer_1] (hidden_layer_2) {Hidden Layer 2};\n        \\node[draw, rectangle, right=of hidden_layer_2] (features) {Features for Deep Kernel GP $\\psi$};\n        \\node[draw, rectangle, below=of features] (kernel) {Kernel $\\mathbf{K}$ from Equation~\\ref{eq:weight_kernel}};\n        \\node[draw, rectangle, below=of kernel] (output) {Predictive mean and variance, $\\mu(\\mathbf{x}_c)$, $\\sigma^2(\\mathbf{x}_c)$ from Equation~\\ref{eq:weight_kernel}};\n        \n        % Arrows\n        \\draw[->] (config) -- (hidden_layer_1);\n        \\draw[->] (budget) -- (hidden_layer_1);\n        \\draw[->] (learning_curve) -- (hidden_layer_1);\n        \\draw[->] (hidden_layer_1) -- (hidden_layer_2);\n        \\draw[->] (hidden_layer_2) -- (features);\n        \\draw[->] (features) -- (kernel);\n        \\draw[->] (kernel) -- (output);\n        \n        % Additional nodes and arrows\n        \\node[draw, circle, below=of hidden_layer_1] (conv) {Conv};\n        \\node[draw, circle, right=of conv] (max_pool) {Max};\n        \\node[draw, circle, right=of max_pool, fill=blue!20] (readout) {Readout};\n        \n        \\draw[->] (hidden_layer_1) -- (conv);\n        \\draw[->] (conv) -- (max_pool);\n        \\draw[->] (max_pool) -- (readout);\n        \n        \\draw[->, blue, dashed, thick] (readout) -- (hidden_layer_2);\n        \\draw[->, blue, dashed, thick] (readout) -- (features);\n        \n        % Blue box around PIGMN\n        \\node[draw, rectangle, rounded corners, fill=blue!20, minimum width=2cm, minimum height=1cm, above=of checkpoint, anchor=south] (pigmn) {PIGMN};\n        \\draw[->, blue, thick] (checkpoint) -- (pigmn);\n    \\end{tikzpicture}\n    \\caption{We show an overview of our method, Forecasting Model Search (FMS), which builds on DyHPO's multifidelity method from Algorithm~\\ref{alg:dyhpo}. Novel components of FMS are highlighted in \\textcolor{blue}{blue} and further detailed in Algorithm~\\ref{alg:fms}. We include DyHPO's features from the hyperparameter configuration, budget, and learning curve \\cite{wistuba2023supervising}. Notably, we also featurize the model's checkpointed weights $\\mathbf{W}$ with a permutation-invariant graph metanetwork (PIGMN) as in Section~\\ref{pigms} for input to a deep kernel GP (see Equation~\\ref{deepkernelgp}/\\ref{eq:weight_kernel}). This provides the HPO with an -- often pre-existing -- rich source of information, which implicitly includes the architecture, dataset, loss, and optimization process. FMS shows improved predictions about hyperparameter performance across compute budgets (see Table~\\ref{ktau-values}), improved quality of the final selected configuration across compute budgets (see Figure~\\ref{fig:regret_over_time}), and a potential to generalize beyond what was seen in training (see Figure~\\ref{fig:fms-transfer}). Specific design choices for this surrogate model are detailed in Appendix Section~\\ref{meta-hyperparams}.}\n    \\label{fig:fms-diagram}\n\\end{figure}\n\n\\end{document}\n```\n\nThis code will produce a figure similar to the one described in the caption. Make sure to include the necessary packages and adjust the positioning and styles as needed. The `tikz` library is used for drawing the diagram, and the `hyperref` package is included for creating links in the caption.",
  "latex_code": "\\documentclass{article}\n\\usepackage{amsmath}\n\\usepackage{graphicx}\n\\usepackage{hyperref}\n\\usepackage{tikz}\n\\usetikzlibrary{arrows.meta, positioning, shapes.geometric}\n\n\\begin{document}\n\n\\begin{figure}[h]\n    \\centering\n    \\begin{tikzpicture}[node distance=1cm, auto]\n        % Nodes\n        \\node[draw, rectangle] (config) {Hyperparameter Config $\\mathbf{x}$};\n        \\node[draw, rectangle, below=of config] (budget) {Budget $r$};\n        \\node[draw, rectangle, below=of budget] (learning_curve) {Learning Curve $\\mathbf{Y}_{1:T}$};\n        \\node[draw, rectangle, below=of learning_curve, fill=blue!20] (checkpoint) {Model Checkpoint $\\mathbf{W}$};\n        \\node[draw, circle, right=of config] (hidden_layer_1) {Hidden Layer 1};\n        \\node[draw, circle, right=of hidden_layer_1] (hidden_layer_2) {Hidden Layer 2};\n        \\node[draw, rectangle, right=of hidden_layer_2] (features) {Features for Deep Kernel GP $\\psi$};\n        \\node[draw, rectangle, below=of features] (kernel) {Kernel $\\mathbf{K}$ from Equation~\\ref{eq:weight_kernel}};\n        \\node[draw, rectangle, below=of kernel] (output) {Predictive mean and variance, $\\mu(\\mathbf{x}_c)$, $\\sigma^2(\\mathbf{x}_c)$ from Equation~\\ref{eq:weight_kernel}};\n        \n        % Arrows\n        \\draw[->] (config) -- (hidden_layer_1);\n        \\draw[->] (budget) -- (hidden_layer_1);\n        \\draw[->] (learning_curve) -- (hidden_layer_1);\n        \\draw[->] (hidden_layer_1) -- (hidden_layer_2);\n        \\draw[->] (hidden_layer_2) -- (features);\n        \\draw[->] (features) -- (kernel);\n        \\draw[->] (kernel) -- (output);\n        \n        % Additional nodes and arrows\n        \\node[draw, circle, below=of hidden_layer_1] (conv) {Conv};\n        \\node[draw, circle, right=of conv] (max_pool) {Max};\n        \\node[draw, circle, right=of max_pool, fill=blue!20] (readout) {Readout};\n        \n        \\draw[->] (hidden_layer_1) -- (conv);\n        \\draw[->] (conv) -- (max_pool);\n        \\draw[->] (max_pool) -- (readout);\n        \n        \\draw[->, blue, dashed, thick] (readout) -- (hidden_layer_2);\n        \\draw[->, blue, dashed, thick] (readout) -- (features);\n        \n        % Blue box around PIGMN\n        \\node[draw, rectangle, rounded corners, fill=blue!20, minimum width=2cm, minimum height=1cm, above=of checkpoint, anchor=south] (pigmn) {PIGMN};\n        \\draw[->, blue, thick] (checkpoint) -- (pigmn);\n    \\end{tikzpicture}\n    \\caption{We show an overview of our method, Forecasting Model Search (FMS), which builds on DyHPO's multifidelity method from Algorithm~\\ref{alg:dyhpo}. Novel components of FMS are highlighted in \\textcolor{blue}{blue} and further detailed in Algorithm~\\ref{alg:fms}. We include DyHPO's features from the hyperparameter configuration, budget, and learning curve \\cite{wistuba2023supervising}. Notably, we also featurize the model's checkpointed weights $\\mathbf{W}$ with a permutation-invariant graph metanetwork (PIGMN) as in Section~\\ref{pigms} for input to a deep kernel GP (see Equation~\\ref{deepkernelgp}/\\ref{eq:weight_kernel}). This provides the HPO with an -- often pre-existing -- rich source of information, which implicitly includes the architecture, dataset, loss, and optimization process. FMS shows improved predictions about hyperparameter performance across compute budgets (see Table~\\ref{ktau-values}), improved quality of the final selected configuration across compute budgets (see Figure~\\ref{fig:regret_over_time}), and a potential to generalize beyond what was seen in training (see Figure~\\ref{fig:fms-transfer}). Specific design choices for this surrogate model are detailed in Appendix Section~\\ref{meta-hyperparams}.}\n    \\label{fig:fms-diagram}\n\\end{figure}\n\n\\end{document}",
  "ground_truth": "\\documentclass{article}\n\\usepackage[utf8]{inputenc}\n\\usepackage[T1]{fontenc}\n\\usepackage{xcolor}\n\\usepackage{tikz}\n\\usetikzlibrary{positioning, shapes.geometric, arrows}\n\\usetikzlibrary{positioning,calc,bayesnet,external}\n\\usepackage{pgfplots}\n\\usepackage{amsmath}\n\n\\begin{document}\n\n\\begin{tikzpicture}[\n  node distance=1.5cm,\n  >=stealth',\n  every node/.style={align=center, font=\\fontsize{9}{0}\\selectfont},\n]\n\n\\tikzstyle{boxstyle}=[rectangle,draw=black,minimum size=15]\n\\tikzstyle{circlestyle}=[circle,draw=black,minimum size=15]\n\\tikzstyle{roundedrect}=[rounded rectangle,draw=black,minimum size=15]\n\\tikzstyle{blueroundedrect}=[rounded rectangle,draw=blue,minimum size=15]\n\\tikzstyle{bluerectangle}=[rectangle,draw=blue,minimum size=15]\n\\tikzstyle{bluearrow}=[->, draw=blue]\n\n\\node[boxstyle](x_input) at (0, 2) {Hyperparameter Config $\\textbf{x}_{i}$};\n\\node[boxstyle](b_input) at (0, 1) {Budget $j$};\n\\node[boxstyle](lc_input) at (0, 0) {Learning Curve $\\mathbf{Y}_{i,j-1}$};\n\\node[bluerectangle](pigm_input) at (0, -1) {\\textcolor{blue}{Model Checkpoint $\\mathbf{W}$}};\n\n\n\\node[circlestyle](layer1) at (3, 1.5) {Hidden\\\\Layer 1};\n\\node[circlestyle](layer2) at (7, 1.5) {Hidden\\\\Layer 2}; \n\n\\node[roundedrect](conv) at (3.5, 0) {Conv}; \n\\node[roundedrect](max) at (5, 0) {Max}; \n\\node[blueroundedrect](pigm) at (3.5, -1) {\\textcolor{blue}{PIGMN}};\n\\node[blueroundedrect](readout) at (6, -1) {\\textcolor{blue}{Readout}};\n\n\\draw[->] (x_input) -- (layer1);\n\\draw[->] (b_input) -- (layer1);\n\\draw[->] (lc_input) -- (conv);\n\\draw[bluearrow] (pigm_input) -- (pigm);\n\\draw[->] (layer1) -- (layer2);\n\\draw[->] (conv) -- (max);\n\\draw[->] (max) -- (layer2);\n\\draw[bluearrow] (pigm) -- (readout);\n\\draw[bluearrow] (readout) -- (layer2);\n\n\n\\node[boxstyle, right=of layer2] (output) {Features for\\\\Deep Kernel GP $\\psi$};\n\\draw[->] (layer2) -- (output);\n\n\\node[boxstyle, below=of output, yshift=1cm] (kernelOutput) {Kernel $\\mathbf{K}$ from Equation~\\ref{eq:weight_kernel}};\n\\draw[->] (output) -- (kernelOutput);\n\n\\node[boxstyle, below=of kernelOutput, yshift=1cm] (mean_var_Output) {Predictive mean and variance\\\\ $\\mu(\\mathbf{x}_{i}), \\sigma^2(\\mathbf{x}_{i})$ from Equation~\\ref{eq:pred_mean_var}};\n\\draw[->] (kernelOutput) -- (mean_var_Output);\n\n\\end{tikzpicture}\n\n\\end{document}"
}