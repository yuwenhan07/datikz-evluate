{
  "prompt": "\\textbf{Accuracy comparison of Continued Pretrained models on English (Left, Right) and Turkish (Right) question answering tasks and demonstrating the original language catastrophic forgetting while learning the new language.} In the table on the left, the performance of our Hamza$_{\\scriptsize Mistral}$ and Hamza$_{\\scriptsize GPT2-xl}$ models that are adapted on Turkish together with the original Mistral 7B and GPT2-xl. We present the result of our ablation study, where the performance of the adapted models is given by progressively enlarging the pretraining corpus size from 0.1 GB to 5 GB. Here, the zero and few-show accuracies were evaluated on the original ARC and TruthfulQA. The figure on the right illustrates the Mistral model's results on both Turkish and English versions of the ARC dataset, highlighting its improved performance in Turkish and decreasing performance in English with continued pretraining.",
  "response": "To generate LaTeX code for the provided image and caption, you can use the `pgfplots` package for plotting and the `caption` package for captions. Below is an example of how you might structure your LaTeX document:\n\n```latex\n\\documentclass{article}\n\\usepackage{pgfplots}\n\\pgfplotsset{compat=1.16}\n\\usepackage{caption}\n\n\\begin{document}\n\n\\begin{figure}[h]\n    \\centering\n    \\begin{tikzpicture}\n        \\begin{axis}[\n            title={Comparison of ARC Scores of Mistral Models},\n            xlabel={\\# of Turkish Tokens},\n            ylabel={English \\& Turkish ARC Acc.},\n            xmin=0, xmax=3,\n            ymin=30, ymax=65,\n            xtick={0, 0.5, 1, 1.5, 2, 2.5},\n            xticklabels={0, 0.5B, 1B, 1.5B, 2B, 2.5B},\n            ytick={30, 35, 40, 45, 50, 55, 60},\n            legend pos=north east,\n            ymajorgrids=true,\n            grid style=dashed,\n        ]\n        \\addplot[\n            color=blue,\n            mark=o,\n            ]\n            coordinates {\n                (0, 32.5)(0.5, 33.5)(1, 36.5)(1.5, 37.5)(2, 39.5)(2.5, 40)\n            };\n            \\addlegendentry{ARC-tr Acc.}\n        \\addplot[\n            color=red,\n            mark=square,\n            ]\n            coordinates {\n                (0, 62)(0.5, 56)(1, 53)(1.5, 52)(2, 50)(2.5, 46)\n            };\n            \\addlegendentry{ARC-eng Acc.}\n        \\end{axis}\n    \\end{tikzpicture}\n    \\caption{Accuracy comparison of Continued Pretrained models on English (Left, Right) and Turkish (Right) question answering tasks and demonstrating the original language catastrophic forgetting while learning the new language. In the table on the left, the performance of our Hamza$_{\\scriptsize Mistral}$ and Hamza$_{\\scriptsize GPT2-xl}$ models that are adapted on Turkish together with the original Mistral 7B and GPT2-xl. We present the result of our ablation study, where the performance of the adapted models is given by progressively enlarging the pretraining corpus size from 0.1 GB to 5 GB. Here, the zero and few-show accuracies were evaluated on the original ARC and TruthfulQA. The figure on the right illustrates the Mistral model's results on both Turkish and English versions of the ARC dataset, highlighting its improved performance in Turkish and decreasing performance in English with continued pretraining.}\n    \\label{fig:arc_scores}\n\\end{figure}\n\n\\end{document}\n```\n\nThis code will create a figure similar to the one described in the caption. Make sure to include the necessary packages in your preamble:\n\n```latex\n\\usepackage{pgfplots}\n\\pgfplotsset{compat=1.16}\n\\usepackage{caption}\n```\n\nThis setup will produce a figure with the specified title, labels, and legend, along with the detailed caption as described.",
  "latex_code": "\\documentclass{article}\n\\usepackage{pgfplots}\n\\pgfplotsset{compat=1.16}\n\\usepackage{caption}\n\n\\begin{document}\n\n\\begin{figure}[h]\n    \\centering\n    \\begin{tikzpicture}\n        \\begin{axis}[\n            title={Comparison of ARC Scores of Mistral Models},\n            xlabel={\\# of Turkish Tokens},\n            ylabel={English \\& Turkish ARC Acc.},\n            xmin=0, xmax=3,\n            ymin=30, ymax=65,\n            xtick={0, 0.5, 1, 1.5, 2, 2.5},\n            xticklabels={0, 0.5B, 1B, 1.5B, 2B, 2.5B},\n            ytick={30, 35, 40, 45, 50, 55, 60},\n            legend pos=north east,\n            ymajorgrids=true,\n            grid style=dashed,\n        ]\n        \\addplot[\n            color=blue,\n            mark=o,\n            ]\n            coordinates {\n                (0, 32.5)(0.5, 33.5)(1, 36.5)(1.5, 37.5)(2, 39.5)(2.5, 40)\n            };\n            \\addlegendentry{ARC-tr Acc.}\n        \\addplot[\n            color=red,\n            mark=square,\n            ]\n            coordinates {\n                (0, 62)(0.5, 56)(1, 53)(1.5, 52)(2, 50)(2.5, 46)\n            };\n            \\addlegendentry{ARC-eng Acc.}\n        \\end{axis}\n    \\end{tikzpicture}\n    \\caption{Accuracy comparison of Continued Pretrained models on English (Left, Right) and Turkish (Right) question answering tasks and demonstrating the original language catastrophic forgetting while learning the new language. In the table on the left, the performance of our Hamza$_{\\scriptsize Mistral}$ and Hamza$_{\\scriptsize GPT2-xl}$ models that are adapted on Turkish together with the original Mistral 7B and GPT2-xl. We present the result of our ablation study, where the performance of the adapted models is given by progressively enlarging the pretraining corpus size from 0.1 GB to 5 GB. Here, the zero and few-show accuracies were evaluated on the original ARC and TruthfulQA. The figure on the right illustrates the Mistral model's results on both Turkish and English versions of the ARC dataset, highlighting its improved performance in Turkish and decreasing performance in English with continued pretraining.}\n    \\label{fig:arc_scores}\n\\end{figure}\n\n\\end{document}",
  "ground_truth": "\\documentclass{article}\n\\usepackage[breakable]{tcolorbox}\n\\usepackage{amsmath}\n\\usepackage{pgfplots}\n\\pgfplotsset{compat=1.17}\n\n\\begin{document}\n\n\\begin{tikzpicture}\n\\begin{axis}[\n    title={\\textbf{Comparison of ARC Scores of Mistral Model}},\n    xlabel={\\textbf{\\# of Turkish Tokens}},\n    ylabel={\\textbf{\\textcolor{red}{English} \\&  \\textcolor{blue}{Turkish} ARC Acc.}},\n    xtick={0,1,2,3,4,5,6},\n    xticklabels={\n        0,\n        50M,\n        130M,\n        250M,\n        0.5B,\n        1.1B,\n        2.5B,\n    },\n    xlabel style={at={(0.5,-0.10)}},\n    legend pos=outer north east,\n    ymajorgrids=true,\n    grid style=dashed,\n    legend style={cells={align=left}},\n    width=0.75\\textwidth,\n    xtick distance=5,\n    legend style={at={(0.97,0.88)},anchor=east},\n]\n\n% Adding data for Turkish ARC\n\\addplot[\n    color=blue,\n    mark=o,\n    thick,\n    nodes near coords,\n    point meta=explicit symbolic,\n] coordinates {\n    (0,32.68)\n    (1,33.70)\n    (2,36.86)\n    (3,36.01)\n    (4,38.57)\n    (5,38.91)\n    (6,39.85)\n};\n\n% Adding data for English ARC\n\\addplot[\n    color=red,\n    mark=square,\n    thick,\n    nodes near coords,\n    point meta=explicit symbolic,\n] coordinates {\n    (0,61.52)\n    (1,56.14)\n    (2,52.90)\n    (3,52.39)\n    (4,51.71)\n    (5,49.32)\n    (6,45.90)\n};\n\n\\legend{ARC-tr Acc., ARC-eng Acc.}\n\\end{axis}\n\\end{tikzpicture}\n\n\\end{document}"
}