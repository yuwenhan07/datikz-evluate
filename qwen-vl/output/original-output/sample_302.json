{
  "prompt": "We vary the percentage of selected data from the Alpaca-Data dataset to train the OPT 6.7B model and conduct a comparison of the win rates obtained when the data is self-selected by the 6.7B model versus selected by smaller models. The smaller model used is mentioned on the X-axis and the win rate is on the Y-axis.",
  "response": "To conduct this analysis, you'll need to follow several steps:\n\n1. **Prepare the Data**: Ensure that you have the Alpaca-Data dataset and the OPT 6.7B model available for training.\n\n2. **Train the OPT 6.7B Model**: Train the OPT 6.7B model using different percentages of the Alpaca-Data dataset. This will give you a baseline performance for each percentage of data used.\n\n3. **Self-Selection by the 6.7B Model**: For each percentage of data, use the trained OPT 6.7B model to select its own subset of data. This can be done by having the model generate predictions or outputs on a held-out validation set and then selecting the top-performing examples based on some metric (e.g., accuracy, perplexity).\n\n4. **Comparison with Smaller Models**: Use smaller models (mentioned on the X-axis) to select subsets of the Alpaca-Data dataset and compare their win rates against the self-selected subsets by the 6.7B model.\n\n5. **Collect Win Rates**: For each percentage of data, collect the win rates of both the self-selected subsets by the 6.7B model and the subsets selected by the smaller models. Plot these win rates on the Y-axis against the smaller models' sizes on the X-axis.\n\nHere's a step-by-step guide to implement this in Python:\n\n### Step 1: Prepare the Data\n\nAssuming you have the Alpaca-Data dataset and the OPT 6.7B model:\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom sklearn.model_selection import train_test_split\n\n# Load the tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/opt-6.7b\")\nmodel = AutoModelForCausalLM.from_pretrained(\"EleutherAI/opt-6.7b\")\n\n# Load the Alpaca-Data dataset\n# Assuming it's in a pandas DataFrame format\nalpaca_data = pd.read_csv('path_to_alpaca_data.csv')\n\n# Tokenize the data\ndef tokenize_function(examples):\n    return tokenizer(examples['text'], truncation=True)\n\ntokenized_datasets = alpaca_data.map(tokenize_function, batched=True)\ntrain_dataset, val_dataset = train_test_split(tokenized_datasets, test_size=0.1, shuffle=True)\n```\n\n### Step 2: Train the OPT 6.7B Model\n\n```python\n# Define the training function\ndef train_model(model, train_dataset, val_dataset, epochs=10, batch_size=8):\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n    criterion = torch.nn.CrossEntropyLoss()\n\n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0\n        for batch in DataLoader(train_dataset, batch_size=batch_size, shuffle=True):\n            input_ids = batch['input_ids'].to(device)\n            labels = batch['labels'].to(device)\n            outputs = model(input_ids=input_ids, labels=labels)\n            loss = outputs.loss\n            total_loss += loss.item()\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        \n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for batch in DataLoader(val_dataset, batch_size=batch_size):\n                input_ids = batch['input_ids'].to(device)\n                labels = batch['labels'].to(device)\n                outputs = model(input_ids=input_ids, labels=labels)\n                loss = outputs.loss\n                val_loss += loss.item()\n        \n        print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {total_loss/len(train_dataset)}, Validation Loss: {val_loss/len(val_dataset)}\")\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Train the model\ntrain_model(model, train_dataset, val_dataset)\n```\n\n### Step 3: Self-Selection by the 6.7B Model\n\n```python\ndef self_select(data, model, num_samples):\n    # Tokenize the data\n    tokenized_data = tokenizer(data['text'], truncation=True)\n    \n    # Convert to PyTorch tensors\n    input_ids = torch.tensor(tokenized_data['input_ids']).to(device)\n    labels = torch.tensor(tokenized_data['labels']).to(device)\n    \n    # Generate predictions\n    with torch.no_grad():\n        outputs = model(input_ids=input_ids, labels=labels)\n        logits = outputs.logits\n    \n    # Select top samples\n    _, indices = torch.topk(logits, k=num_samples, dim=-1)\n    selected_data = data.iloc[indices]\n    \n    return selected_data\n\n# Example usage\nselected_data = self_select(alpaca_data, model, 1000)\n```\n\n### Step 4: Comparison with Smaller Models\n\n```python\n# Define the smaller models\nsmaller_models = [\"EleutherAI/gpt-neo-125M\", \"EleutherAI/gpt-neo-2.7B\"]\nwin_rates = []\n\nfor model_name in smaller_models:\n    # Load the smaller model\n    small_model = AutoModelForCausalLM.from_pretrained(model_name)\n    small_model.to(device)\n    \n    # Tokenize the data\n    tokenized_data = tokenizer(alpaca_data['text'], truncation=True)\n    \n    # Convert to PyTorch tensors\n    input_ids = torch.tensor(tokenized_data['input_ids']).to(device)\n    labels = torch.tensor(tokenized_data['labels']).to(device)\n    \n    # Generate predictions\n    with torch.no_grad():\n        outputs = small_model(input_ids=input_ids, labels=labels)\n        logits = outputs.logits\n    \n    # Select top samples\n    _, indices = torch.topk(logits, k=1000, dim=-1)\n    selected_data = alpaca_data.iloc[indices]\n    \n    # Compare with the self-selected data\n    # Calculate win rate\n    # Assuming win_rate is defined elsewhere\n    win_rate = calculate_win_rate(selected_data, self_selected_data)\n    win_rates.append(win_rate)\n\n# Plot the results\nplt.plot(smaller_models, win_rates)\nplt.xlabel('Smaller Model Size')\nplt.ylabel('Win Rate')\nplt.title('Comparison of Win Rates')\nplt.show()\n```\n\n### Step 5: Collect Win Rates and Plot\n\n```python\ndef calculate_win_rate(smaller_model_data, self_selected_data):\n    # Implement your win rate calculation logic here\n    pass\n\n# Example usage\nwin_rates = []\nfor percentage in [10, 20, 30, 40, 50]:\n    # Get the data for the current percentage\n    data_percentage = alpaca_data.sample(frac=percentage / 100)\n    \n    # Self-select using the 6.7B model\n    self_selected_data = self_select(data_percentage, model, 1000)\n    \n    # Select using smaller models\n    win_rates.append(calculate_win_rate(data_percentage, self_selected_data))\n\n# Plot the results\nplt.plot(smaller_models, win_rates)\nplt.xlabel('Smaller Model Size')\nplt.ylabel('Win Rate')\nplt.title('Comparison of Win Rates')\nplt.show()\n```\n\nThis script provides a basic framework for conducting the analysis. You may need to adjust the specifics depending on how you define the win rate and the exact details of your dataset and models.",
  "latex_code": "python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom sklearn.model_selection import train_test_split\n\n# Load the tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/opt-6.7b\")\nmodel = AutoModelForCausalLM.from_pretrained(\"EleutherAI/opt-6.7b\")\n\n# Load the Alpaca-Data dataset\n# Assuming it's in a pandas DataFrame format\nalpaca_data = pd.read_csv('path_to_alpaca_data.csv')\n\n# Tokenize the data\ndef tokenize_function(examples):\n    return tokenizer(examples['text'], truncation=True)\n\ntokenized_datasets = alpaca_data.map(tokenize_function, batched=True)\ntrain_dataset, val_dataset = train_test_split(tokenized_datasets, test_size=0.1, shuffle=True)",
  "ground_truth": "\\documentclass[11pt]{article}\n\\usepackage[T1]{fontenc}\n\\usepackage[utf8]{inputenc}\n\\usepackage{pgfplots}\n\\usepgfplotslibrary{external}\n\\usepgfplotslibrary{colorbrewer}\n\\pgfplotsset{\n    % initialize Dark2\n    cycle list/Dark2,\n    % combine it with 'mark list*':\n    cycle multiindex* list={\n        mark list*\\nextlist\n        Dark2\\nextlist\n    },\n    every axis plot/.append style={line width=1pt},\n}\n\\pgfplotsset{compat=newest}\n\\usetikzlibrary{shapes.geometric}\n\n\\begin{document}\n\n\\begin{tikzpicture}\n\\begin{axis}[\n    symbolic x coords={OPT 350M, OPT 1.3B,OPT 2.7B,OPT 6.7B},\n    xticklabels={\n        350M,\n        1.3B,\n        2.7B,\n        6.7B\\\\(Self-Ranking)\n    },\n    xtick=data,\n    xlabel=$\\mathcal{LP}(1)$ Ranking Source,\n    x tick label style={text width=4cm,align=center},\n    legend style={at={(0.5,1.15)}, anchor=north, legend columns=3, legend cell align={left}, column sep=0.25em},\n    ylabel=Win rate,\n    enlarge y limits=0.1,\n    enlarge x limits=0.07,\n]\n\\addplot coordinates {\n    (OPT 350M,51.24)\n    (OPT 1.3B,51.76)\n    (OPT 2.7B,51.22)\n    (OPT 6.7B,51.08)\n};\n\\addplot coordinates {\n    (OPT 350M,52.01)\n    (OPT 1.3B,52.11)\n    (OPT 2.7B,52.84)\n    (OPT 6.7B,51.49)\n};\n\\addplot coordinates {\n    (OPT 350M,51.61)\n    (OPT 1.3B,52.80)\n    (OPT 2.7B,53.15)\n    (OPT 6.7B,52.16)\n};\n\\legend{10\\% Low, 25\\% Low, 33\\% Low}\n\\end{axis}\n\\end{tikzpicture}\n\n\\end{document}"
}