{
  "prompt": "\\textbf{Accuracy comparison of Continued Pretrained models on English (Left, Right) and Turkish (Right) question answering tasks and demonstrating the original language catastrophic forgetting while learning the new language.} In the table on the left, the performance of our Hamza$_{\\scriptsize Mistral}$ and Hamza$_{\\scriptsize GPT2-xl}$ models that are adapted on Turkish together with the original Mistral 7B and GPT2-xl. We present the result of our ablation study, where the performance of the adapted models is given by progressively enlarging the pretraining corpus size from 0.1 GB to 5 GB. Here, the zero and few-show accuracies were evaluated on the original ARC and TruthfulQA. The figure on the right illustrates the Mistral model's results on both Turkish and English versions of the ARC dataset, highlighting its improved performance in Turkish and decreasing performance in English with continued pretraining.",
  "response": "The provided text discusses an accuracy comparison between different pretrained models, specifically focusing on their performance on English and Turkish question-answering tasks. It also highlights the phenomenon of catastrophic forgetting, which refers to the loss of previously learned knowledge when a model is trained on a new task.\n\n### Key Points:\n\n1. **Models Used**:\n   - Hamza\\(_{\\scriptsize \\text{Mistral}}\\) and Hamza\\(_{\\scriptsize \\text{GPT2-xl}}\\)\n   - Original models: Mistral 7B and GPT2-xl\n\n2. **Ablation Study**:\n   - The performance of the adapted models was evaluated by progressively increasing the pretraining corpus size from 0.1 GB to 5 GB.\n   - Zero-shot and few-shot accuracies were measured on the original ARC and TruthfulQA datasets.\n\n3. **Performance on Turkish and English Tasks**:\n   - The table on the left shows the performance of the adapted models on Turkish and English tasks, alongside the original models.\n   - The figure on the right demonstrates the Mistral model's performance on both Turkish and English versions of the ARC dataset.\n   - The figure illustrates that the Mistral model performs better on the Turkish version of the ARC dataset but shows a decrease in performance on the English version as it continues to be pretrained.\n\n### Summary:\nThe text emphasizes how the adaptation of pretrained models on Turkish improves their performance on the Turkish version of the ARC dataset but leads to a decline in performance on the English version. This phenomenon is indicative of catastrophic forgetting, where the model loses its ability to perform well on the original language after being trained on a new one.",
  "latex_code": "The provided text discusses an accuracy comparison between different pretrained models, specifically focusing on their performance on English and Turkish question-answering tasks. It also highlights the phenomenon of catastrophic forgetting, which refers to the loss of previously learned knowledge when a model is trained on a new task.\n\n### Key Points:\n\n1. **Models Used**:\n   - Hamza\\(_{\\scriptsize \\text{Mistral}}\\) and Hamza\\(_{\\scriptsize \\text{GPT2-xl}}\\)\n   - Original models: Mistral 7B and GPT2-xl\n\n2. **Ablation Study**:\n   - The performance of the adapted models was evaluated by progressively increasing the pretraining corpus size from 0.1 GB to 5 GB.\n   - Zero-shot and few-shot accuracies were measured on the original ARC and TruthfulQA datasets.\n\n3. **Performance on Turkish and English Tasks**:\n   - The table on the left shows the performance of the adapted models on Turkish and English tasks, alongside the original models.\n   - The figure on the right demonstrates the Mistral model's performance on both Turkish and English versions of the ARC dataset.\n   - The figure illustrates that the Mistral model performs better on the Turkish version of the ARC dataset but shows a decrease in performance on the English version as it continues to be pretrained.\n\n### Summary:\nThe text emphasizes how the adaptation of pretrained models on Turkish improves their performance on the Turkish version of the ARC dataset but leads to a decline in performance on the English version. This phenomenon is indicative of catastrophic forgetting, where the model loses its ability to perform well on the original language after being trained on a new one.",
  "ground_truth": "\\documentclass{article}\n\\usepackage[breakable]{tcolorbox}\n\\usepackage{amsmath}\n\\usepackage{pgfplots}\n\\pgfplotsset{compat=1.17}\n\n\\begin{document}\n\n\\begin{tikzpicture}\n\\begin{axis}[\n    title={\\textbf{Comparison of ARC Scores of Mistral Model}},\n    xlabel={\\textbf{\\# of Turkish Tokens}},\n    ylabel={\\textbf{\\textcolor{red}{English} \\&  \\textcolor{blue}{Turkish} ARC Acc.}},\n    xtick={0,1,2,3,4,5,6},\n    xticklabels={\n        0,\n        50M,\n        130M,\n        250M,\n        0.5B,\n        1.1B,\n        2.5B,\n    },\n    xlabel style={at={(0.5,-0.10)}},\n    legend pos=outer north east,\n    ymajorgrids=true,\n    grid style=dashed,\n    legend style={cells={align=left}},\n    width=0.75\\textwidth,\n    xtick distance=5,\n    legend style={at={(0.97,0.88)},anchor=east},\n]\n\n% Adding data for Turkish ARC\n\\addplot[\n    color=blue,\n    mark=o,\n    thick,\n    nodes near coords,\n    point meta=explicit symbolic,\n] coordinates {\n    (0,32.68)\n    (1,33.70)\n    (2,36.86)\n    (3,36.01)\n    (4,38.57)\n    (5,38.91)\n    (6,39.85)\n};\n\n% Adding data for English ARC\n\\addplot[\n    color=red,\n    mark=square,\n    thick,\n    nodes near coords,\n    point meta=explicit symbolic,\n] coordinates {\n    (0,61.52)\n    (1,56.14)\n    (2,52.90)\n    (3,52.39)\n    (4,51.71)\n    (5,49.32)\n    (6,45.90)\n};\n\n\\legend{ARC-tr Acc., ARC-eng Acc.}\n\\end{axis}\n\\end{tikzpicture}\n\n\\end{document}"
}