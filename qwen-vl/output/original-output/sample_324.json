{
  "prompt": "Sampling algorithms choose the next token at each time step~\\(s_i\\) by sampling from the conditional distribution~\\(p_\\theta(\\cdot\\mid\\boldsymbol{y}_{:i-1})\\) and appending it to the context.",
  "response": "The statement you provided describes a fundamental aspect of how sampling-based generative models, such as those used in natural language processing (NLP), work. Let's break down the key components:\n\n1. **Time Step \\( s_i \\)**: This refers to the current position in the sequence where the model is making its prediction.\n\n2. **Conditional Distribution \\( p_\\theta(\\cdot \\mid \\boldsymbol{y}_{:i-1}) \\)**: This is the probability distribution over possible tokens given the history of tokens up to the previous time step (\\( \\boldsymbol{y}_{:i-1} \\)). The symbol \\( \\boldsymbol{y}_{:i-1} \\) represents the sequence of tokens observed so far, up to but not including the current time step \\( i \\).\n\n3. **Sampling**: At each time step \\( s_i \\), the model samples the next token from this conditional distribution. This means that the model chooses the next token based on the probabilities assigned by the distribution \\( p_\\theta(\\cdot \\mid \\boldsymbol{y}_{:i-1}) \\).\n\n4. **Appending to Context**: After sampling the next token, it is appended to the existing sequence of tokens (\\( \\boldsymbol{y}_{:i-1} \\)) to form the new context for the next time step. This process continues until the entire sequence is generated or a stopping criterion is met.\n\nIn summary, the sampling algorithm at each time step \\( s_i \\) selects the next token by drawing from the conditional probability distribution over all possible tokens given the history up to that point, and then adds this token to the growing sequence. This iterative process allows the model to generate sequences that are consistent with the learned patterns in the training data.",
  "latex_code": "The statement you provided describes a fundamental aspect of how sampling-based generative models, such as those used in natural language processing (NLP), work. Let's break down the key components:\n\n1. **Time Step \\( s_i \\)**: This refers to the current position in the sequence where the model is making its prediction.\n\n2. **Conditional Distribution \\( p_\\theta(\\cdot \\mid \\boldsymbol{y}_{:i-1}) \\)**: This is the probability distribution over possible tokens given the history of tokens up to the previous time step (\\( \\boldsymbol{y}_{:i-1} \\)). The symbol \\( \\boldsymbol{y}_{:i-1} \\) represents the sequence of tokens observed so far, up to but not including the current time step \\( i \\).\n\n3. **Sampling**: At each time step \\( s_i \\), the model samples the next token from this conditional distribution. This means that the model chooses the next token based on the probabilities assigned by the distribution \\( p_\\theta(\\cdot \\mid \\boldsymbol{y}_{:i-1}) \\).\n\n4. **Appending to Context**: After sampling the next token, it is appended to the existing sequence of tokens (\\( \\boldsymbol{y}_{:i-1} \\)) to form the new context for the next time step. This process continues until the entire sequence is generated or a stopping criterion is met.\n\nIn summary, the sampling algorithm at each time step \\( s_i \\) selects the next token by drawing from the conditional probability distribution over all possible tokens given the history up to that point, and then adds this token to the growing sequence. This iterative process allows the model to generate sequences that are consistent with the learned patterns in the training data.",
  "ground_truth": "\\documentclass[10pt]{article}\n\\usepackage{amsmath,amsfonts,bm}\n\\usepackage{amsmath}\n\\usepackage{amssymb}\n\\usepackage{tikz}\n\\usepackage{pgfplots}\n\\usepackage{xcolor}\n\\usetikzlibrary{shapes, positioning, fit, backgrounds, tikzmark}\n\\pgfplotsset{compat=newest}\n\\usepgfplotslibrary{fillbetween}\n\\usepackage{color-edits}\n\n\\begin{document}\n\n\\begin{tikzpicture}[\n    vocab/.style={\n      draw,\n      text=gray,\n      rectangle split,\n      rectangle split parts=5,\n      rectangle split part align=left,\n      rectangle split draw splits=false,\n    }\n  ]\n  \\node[\n    vocab,\n    rectangle split part fill={none, none, black, none},\n    label=below:\\(s_1\\),\n    label=above:\\(y_1\\sim p_\\theta(\\cdot)\\),\n  ] (s1)  {\n    \\nodepart{one} {boyfriend}\n    \\nodepart{two} {is}\n    \\nodepart{three} \\textcolor{white}{Karma}\n    \\nodepart{four} {my}\n    \\nodepart{five} {\\ldots}\n  };\n  \\node[\n    vocab,\n    rectangle split part fill={none, black, none},\n    label=below:\\(s_2\\),\n    label=above:\\(y_2\\sim p_\\theta(\\cdot\\mid\\boldsymbol{y}_{:1})\\),\n  ] (s2) [right=of s1] {\n    \\nodepart{one} {boyfriend}\n    \\nodepart{two} \\textcolor{white}{is}\n    \\nodepart{three} {Karma}\n    \\nodepart{four} {my}\n    \\nodepart{five} {\\ldots}\n  };\n  \\foreach \\part in {text west, two west, three west, four west, five west} {\n    \\draw[->, shorten >= 4pt] (s1.three east) -- (s2.\\part);\n  }\n  \\node[\n    vocab,\n    rectangle split part fill={none, none, none, black, none},\n    label=below:\\(s_3\\),\n    label=above:\\(y_3\\sim p_\\theta(\\cdot\\mid\\boldsymbol{y}_{:2})\\),\n  ] (s3) [right=of s2] {\n    \\nodepart{one} {boyfriend}\n    \\nodepart{two} {is}\n    \\nodepart{three} {Karma}\n    \\nodepart{four} \\textcolor{white}{my}\n    \\nodepart{five} {\\ldots}\n  };\n  \\foreach \\part in {text west, two west, three west, four west, five west} {\n    \\draw[->, shorten >= 4pt] (s2.two east) -- (s3.\\part);\n  }\n  \\node[\n    vocab,\n    rectangle split part fill={black, none},\n    label=below:\\(s_4\\),\n    label=above:\\(y_4\\sim p_\\theta(\\cdot\\mid\\boldsymbol{y}_{:3})\\),\n  ] (s4) [right=of s3] {\n    \\nodepart{one} \\textcolor{white}{boyfriend}\n    \\nodepart{two} {is}\n    \\nodepart{three} {Karma}\n    \\nodepart{four} {my}\n    \\nodepart{five} {\\ldots}\n  };\n  \\foreach \\part in {text west, two west, three west, four west, five west} {\n    \\draw[->, shorten >= 4pt] (s3.four east) -- (s4.\\part);\n  }\n  \\node[draw, fit=(s1) (s2) (s3) (s4), inner sep=16pt, rounded corners] (decoding) {};\n  \\node[\n    draw=white,\n    rectangle split,\n    rectangle split horizontal,\n    rectangle split parts=4,\n    rectangle split part fill=black,\n    label=\\(\\boldsymbol{y}\\in\\mathcal{Y}\\),\n  ] (seq) [right=of decoding] {};\n  \\draw[->, shorten >= 4pt] (decoding) -- (seq);\n\\end{tikzpicture}\n\n\\end{document}"
}