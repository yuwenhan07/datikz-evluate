{
  "prompt": "We show an overview of our method, Forecasting Model Search (FMS), which builds on DyHPO's multifidelity method from Algorithm~\\ref{alg:dyhpo}. Novel components of FMS are highlighted in {\\color{blue}blue} and further detailed in Algorithm~\\ref{alg:fms}. We include DyHPO's features from the hyperparameter configuration, budget, and learning curve \\cite{wistuba2023supervising}. Notably, we also featurize the model's checkpointed weights $\\mathbf{W}$ with a permutation-invariant graph metanetwork (PIGMN) as in Section~\\ref{pigms} for input to a deep kernel GP (see Equation~\\ref{deepkernelgp}/\\ref{eq:weight_kernel}). This provides the HPO with an -- often pre-existing -- rich source of information, which implicitly includes the architecture, dataset, loss, and optimization process. FMS shows improved predictions about hyperparameter performance across compute budgets (see Table~\\ref{ktau-values}), improved quality of the final selected configuration across compute budgets (see Figure~\\ref{fig:regret_over_time}), and a potential to generalize beyond what was seen in training (see Figure~\\ref{fig:fms-transfer}). Specific design choices for this surrogate model are detailed in Appendix Section~\\ref{meta-hyperparams}.",
  "final_latex_code": "\\documentclass{article}\n\\usepackage{amsmath}\n\\usepackage{graphicx}\n\\usepackage{xcolor}\n\n\\begin{document}\n\n\\section*{Overview of Forecasting Model Search (FMS)}\n\nWe present an overview of our method, Forecasting Model Search (FMS), which builds upon DyHPO's multifidelity method from Algorithm~\\ref{alg:dyhpo}. Novel components of FMS are highlighted in blue and further detailed in Algorithm~\\ref{alg:fms}. We include DyHPO's features from the hyperparameter configuration, budget, and learning curve \\cite{wistuba2023supervising}. Notably, we also featurize the model's checkpointed weights $\\mathbf{W}$ using a permutation-invariant graph metanetwork (PIGMN) as described in Section~\\ref{pigms}, and input it into a deep kernel Gaussian process (GP) (Equation~\\ref{deepkernelgp}/\\ref{eq:weight_kernel}). This provides the hyperparameter optimization (HPO) with an often pre-existing rich source of information that implicitly includes the architecture, dataset, loss, and optimization process.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=\\textwidth]{path_to_your_image.png}\n    \\caption{An overview of our method, Forecasting Model Search (FMS). Novel components are highlighted in blue.}\n    \\label{fig:overview_fms}\n\\end{figure}\n\nFMS demonstrates improvements in predicting hyperparameter performance across different compute budgets (see Table~\\ref{ktau-values}), higher quality of the final selected configuration across compute budgets (see Figure~\\ref{fig:regret_over_time}), and a potential to generalize beyond what was seen in training (see Figure~\\ref{fig:fms-transfer}). Specific design choices for the surrogate model are detailed in Appendix Section~\\ref{meta-hyperparams}.\n\n\\end{document}",
  "compiled_successfully": true,
  "ground_truth": "\\documentclass{article}\n\\usepackage[utf8]{inputenc}\n\\usepackage[T1]{fontenc}\n\\usepackage{xcolor}\n\\usepackage{tikz}\n\\usetikzlibrary{positioning, shapes.geometric, arrows}\n\\usetikzlibrary{positioning,calc,bayesnet,external}\n\\usepackage{pgfplots}\n\\usepackage{amsmath}\n\n\\begin{document}\n\n\\begin{tikzpicture}[\n  node distance=1.5cm,\n  >=stealth',\n  every node/.style={align=center, font=\\fontsize{9}{0}\\selectfont},\n]\n\n\\tikzstyle{boxstyle}=[rectangle,draw=black,minimum size=15]\n\\tikzstyle{circlestyle}=[circle,draw=black,minimum size=15]\n\\tikzstyle{roundedrect}=[rounded rectangle,draw=black,minimum size=15]\n\\tikzstyle{blueroundedrect}=[rounded rectangle,draw=blue,minimum size=15]\n\\tikzstyle{bluerectangle}=[rectangle,draw=blue,minimum size=15]\n\\tikzstyle{bluearrow}=[->, draw=blue]\n\n\\node[boxstyle](x_input) at (0, 2) {Hyperparameter Config $\\textbf{x}_{i}$};\n\\node[boxstyle](b_input) at (0, 1) {Budget $j$};\n\\node[boxstyle](lc_input) at (0, 0) {Learning Curve $\\mathbf{Y}_{i,j-1}$};\n\\node[bluerectangle](pigm_input) at (0, -1) {\\textcolor{blue}{Model Checkpoint $\\mathbf{W}$}};\n\n\n\\node[circlestyle](layer1) at (3, 1.5) {Hidden\\\\Layer 1};\n\\node[circlestyle](layer2) at (7, 1.5) {Hidden\\\\Layer 2}; \n\n\\node[roundedrect](conv) at (3.5, 0) {Conv}; \n\\node[roundedrect](max) at (5, 0) {Max}; \n\\node[blueroundedrect](pigm) at (3.5, -1) {\\textcolor{blue}{PIGMN}};\n\\node[blueroundedrect](readout) at (6, -1) {\\textcolor{blue}{Readout}};\n\n\\draw[->] (x_input) -- (layer1);\n\\draw[->] (b_input) -- (layer1);\n\\draw[->] (lc_input) -- (conv);\n\\draw[bluearrow] (pigm_input) -- (pigm);\n\\draw[->] (layer1) -- (layer2);\n\\draw[->] (conv) -- (max);\n\\draw[->] (max) -- (layer2);\n\\draw[bluearrow] (pigm) -- (readout);\n\\draw[bluearrow] (readout) -- (layer2);\n\n\n\\node[boxstyle, right=of layer2] (output) {Features for\\\\Deep Kernel GP $\\psi$};\n\\draw[->] (layer2) -- (output);\n\n\\node[boxstyle, below=of output, yshift=1cm] (kernelOutput) {Kernel $\\mathbf{K}$ from Equation~\\ref{eq:weight_kernel}};\n\\draw[->] (output) -- (kernelOutput);\n\n\\node[boxstyle, below=of kernelOutput, yshift=1cm] (mean_var_Output) {Predictive mean and variance\\\\ $\\mu(\\mathbf{x}_{i}), \\sigma^2(\\mathbf{x}_{i})$ from Equation~\\ref{eq:pred_mean_var}};\n\\draw[->] (kernelOutput) -- (mean_var_Output);\n\n\\end{tikzpicture}\n\n\\end{document}",
  "attempts": 1
}